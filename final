===============================   final code ========================================

Cell - 1 :

# Import necessary libraries
import pandas as pd
import numpy as np
import pickle
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer
from fuzzywuzzy import fuzz
from metaphone import doublemetaphone
from jellyfish import soundex
from sklearn.model_selection import train_test_split

# Load data from Excel
file_path = 'Mathwizzathon_Entity_Matching_Dataset.xlsx'  # Replace with your Excel file path
data = pd.read_excel(file_path)

# List of common titles and honorifics to ignore
titles = {'dr', 'mr', 'mrs', 'ms', 'ph.d.', 'professor', 'sir', 'lady', 'captain', 'major'}

# Preprocessing functions
def normalize_name(name):
    return str(name).lower().replace(".", "").replace("-", "").replace(",", "").strip()

def remove_titles(name):
    parts = normalize_name(name).split()
    return ' '.join(part for part in parts if part not in titles)

def phonetic_encoding(name):
    encoded = doublemetaphone(name)
    return encoded[0] if encoded[0] else encoded[1]

def soundex_encoding(name):
    return soundex(name)

def extract_initials(name):
    parts = name.split()
    return [part[0] for part in parts]

def initials_match(name1, name2):
    initials1 = set(extract_initials(name1))
    initials2 = set(extract_initials(name2))
    return initials1 == initials2

def approximate_nickname_match(name1, name2):
    if len(name1) < len(name2):
        shorter, longer = name1, name2
    else:
        shorter, longer = name2, name1

    start_match = fuzz.partial_ratio(shorter, longer) >= 60
    phonetic_match = phonetic_encoding(shorter) == phonetic_encoding(longer[:len(shorter)])
    return start_match or phonetic_match

def has_common_components(name1, name2):
    components1 = set(name1.split())
    components2 = set(name2.split())
    return components1.issubset(components2) or components2.issubset(components1)

# Similarity functions
def calculate_similarities(name1, name2):
    name1, name2 = normalize_name(name1), normalize_name(name2)
    name1_no_titles = remove_titles(name1)
    name2_no_titles = remove_titles(name2)

    # Early return for strong matches
    if initials_match(name1_no_titles, name2_no_titles):
        return 1.0
    if approximate_nickname_match(name1_no_titles, name2_no_titles):
        return 1.0
    if has_common_components(name1_no_titles, name2_no_titles):
        return 0.9
    if sorted(name1_no_titles.split()) == sorted(name2_no_titles.split()):
        return 0.9

    # Calculate various similarity scores
    phonetic_sim = fuzz.ratio(phonetic_encoding(name1_no_titles), phonetic_encoding(name2_no_titles)) / 100.0
    levenshtein_sim = fuzz.ratio(name1_no_titles, name2_no_titles) / 100.0
    jaro_winkler_sim = fuzz.token_sort_ratio(name1_no_titles, name2_no_titles) / 100.0
    soundex_sim = 1.0 if soundex_encoding(name1_no_titles) == soundex_encoding(name2_no_titles) else 0.0

    vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 3))
    ngram_matrix = vectorizer.fit_transform([name1_no_titles, name2_no_titles])
    cosine_sim = cosine_similarity(ngram_matrix[0:1], ngram_matrix[1:2])[0][0]

    def jaccard_similarity(name1, name2):
        set1, set2 = set(name1.split()), set(name2.split())
        return len(set1.intersection(set2)) / len(set1.union(set2))

    jaccard_sim = jaccard_similarity(name1_no_titles, name2_no_titles)

    # Weighted score calculation with Soundex similarity included
    weighted_score = (0.3 * phonetic_sim +
                      0.3 * levenshtein_sim +
                      0.4 * jaro_winkler_sim +
                      0.4 * cosine_sim +
                      0.1 * jaccard_sim +
                      0.2 * soundex_sim)

    return weighted_score

# Initial matching function
def match_names_from_excel(data, threshold=0.6):
    results = []
    for _, row in data.iterrows():
        name1, name2 = row['Name1'], row['Name2']
        score = calculate_similarities(name1, name2)
        label = 1 if score >= threshold else 0
        results.append((name1, name2, score, label))
    results_df = pd.DataFrame(results, columns=["Name1", "Name2", "Similarity Score", "Predicted Label"])
    return results_df

# Prepare training data
def prepare_training_data(df):
    X, y = [], []
    for _, row in df.iterrows():
        name1, name2 = row['Name1'], row['Name2']
        score = calculate_similarities(name1, name2)
        X.append([score])
        y.append(row['Actual Label'])
    return np.array(X), np.array(y)

# Initial matching
matched_df = match_names_from_excel(data, threshold=0.6)
matched_df['Actual Label'] = data['Label']

# Prepare training data
X, y = prepare_training_data(matched_df)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Save the trained model as a .pkl file
def save_model(model, filename='name_matching_model.pkl'):
    with open(filename, 'wb') as file:
        pickle.dump(model, file)
    print(f"Model saved as {filename}")

save_model(model)

Cell  - 2 :

# Import necessary libraries
import pandas as pd
import pickle

# Load the trained model
def load_model(filename='name_matching_model.pkl'):
    with open(filename, 'rb') as file:
        model = pickle.load(file)
    return model

# Load the model
loaded_model = load_model()

# Predict similarity score and label for two input names
def predict_similarity(model, name1, name2, threshold=0.6):
    # Use the calculate_similarities function from the modelâ€™s feature set (if embedded)
    score = model.calculate_similarities(name1, name2)  # Assuming function is embedded in model.pkl
    label = 1 if score >= threshold else 0
    return round(score, 2), label

# Load new input Excel file
input_file_path = 'new_input_file.xlsx'  # Replace with the path of your new input Excel file
input_data = pd.read_excel(input_file_path)

# Create DataFrame for results
results = []
for _, row in input_data.iterrows():
    name1, name2 = row['Name1'], row['Name2']
    score, label = predict_similarity(loaded_model, name1, name2)
    results.append([name1, name2, score, label])

# Save results to a new Excel file
results_df = pd.DataFrame(results, columns=['Name1', 'Name2', 'Similarity Score', 'Label'])
output_file_path = 'name_matching_results.xlsx'  # Specify desired output file path
results_df.to_excel(output_file_path, index=False)
print(f"Results saved to {output_file_path}")

======================================== approach -2 ===============================================


import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from fuzzywuzzy import fuzz
from metaphone import doublemetaphone

# Load data from Excel
file_path = 'Mathwizzathon_Entity_Matching_Dataset.xlsx'  # Replace with your Excel file path
data = pd.read_excel(file_path)

# Preprocessing functions
def normalize_name(name):
    return str(name).lower().replace(".", "").replace("-", "").replace(",", "").strip()

def phonetic_encoding(name):
    encoded = doublemetaphone(name)
    return encoded[0] if encoded[0] else encoded[1]

# Feature extraction function
def generate_features(name1, name2):
    name1, name2 = normalize_name(name1), normalize_name(name2)
    features = {
        'phonetic_sim': fuzz.ratio(phonetic_encoding(name1), phonetic_encoding(name2)) / 100.0,
        'levenshtein_sim': fuzz.ratio(name1, name2) / 100.0,
        'jaro_winkler_sim': fuzz.token_sort_ratio(name1, name2) / 100.0,
        'cosine_sim': cosine_similarity(
            CountVectorizer(analyzer='char', ngram_range=(2, 3)).fit_transform([name1, name2])[0:1],
            CountVectorizer(analyzer='char', ngram_range=(2, 3)).fit_transform([name1, name2])[1:2])[0][0]
    }
    return list(features.values())

# Prepare feature matrix and labels
X = []
y = []
for _, row in data.iterrows():
    name1, name2, label = row['Name1'], row['Name2'], row['Label']
    X.append(generate_features(name1, name2))
    y.append(label)

# Split data into training (80%) and remaining (20%) sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)

# Split the remaining data into validation (10%) and testing (10%) sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Print dataset sizes
print(f"Training set size: {len(X_train)}")
print(f"Validation set size: {len(X_val)}")
print(f"Testing set size: {len(X_test)}")

# Train classifier on the training dataset
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# Evaluate on training set
y_train_pred = clf.predict(X_train)
train_accuracy = accuracy_score(y_train, y_train_pred)
train_precision = precision_score(y_train, y_train_pred)
train_recall = recall_score(y_train, y_train_pred)

# Evaluate on validation set
y_val_pred = clf.predict(X_val)
val_accuracy = accuracy_score(y_val, y_val_pred)
val_precision = precision_score(y_val, y_val_pred)
val_recall = recall_score(y_val, y_val_pred)

# Evaluate on testing set
y_test_pred = clf.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred)
test_recall = recall_score(y_test, y_test_pred)

# Print metrics for each set
print(f"Training Accuracy: {train_accuracy:.2f}, Precision: {train_precision:.2f}, Recall: {train_recall:.2f}")
print(f"Validation Accuracy: {val_accuracy:.2f}, Precision: {val_precision:.2f}, Recall: {val_recall:.2f}")
print(f"Testing Accuracy: {test_accuracy:.2f}, Precision: {test_precision:.2f}, Recall: {test_recall:.2f}")


===================================================================  approach 3  (without ml ) ===================================================

import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer
from fuzzywuzzy import fuzz
from metaphone import doublemetaphone

# Load data from Excel
file_path = 'Mathwizzathon_Entity_Matching_Dataset.xlsx'  # Replace with your Excel file path
data = pd.read_excel(file_path)

# List of common titles and honorifics to ignore
titles = {'dr', 'mr', 'mrs', 'ms', 'ph.d.', 'professor', 'sir', 'lady', 'captain', 'major'}

# Preprocessing functions
def normalize_name(name):
    # Convert to lowercase, remove punctuations, extra spaces, etc.
    return str(name).lower().replace(".", "").replace("-", "").replace(",", "").strip()

def remove_titles(name):
    # Remove titles and honorifics from the name
    parts = normalize_name(name).split()
    return ' '.join(part for part in parts if part not in titles)

def phonetic_encoding(name):
    # Using Double Metaphone for phonetic similarity
    encoded = doublemetaphone(name)
    return encoded[0] if encoded[0] else encoded[1]

def extract_initials(name):
    """Extract initials from a name."""
    parts = name.split()
    return [part[0] for part in parts]

def initials_match(name1, name2):
    """Check for initials match, regardless of order."""
    initials1 = set(extract_initials(name1))
    initials2 = set(extract_initials(name2))
    return initials1 == initials2

def approximate_nickname_match(name1, name2):
    # Short names match with longer names if they start similarly or are phonetic matches
    if len(name1) < len(name2):
        shorter, longer = name1, name2
    else:
        shorter, longer = name2, name1

    start_match = fuzz.partial_ratio(shorter, longer) >= 75
    phonetic_match = phonetic_encoding(shorter) == phonetic_encoding(longer[:len(shorter)])
    return start_match or phonetic_match

def has_common_components(name1, name2):
    # Check if shorter name is a subset of the longer one
    components1 = set(name1.split())
    components2 = set(name2.split())
    return components1.issubset(components2) or components2.issubset(components1)

# Similarity functions
def calculate_similarities(name1, name2):
    # Normalize names
    name1, name2 = normalize_name(name1), normalize_name(name2)

    # Remove titles from names
    name1_no_titles = remove_titles(name1)
    name2_no_titles = remove_titles(name2)

    # Check for initials match
    if initials_match(name1_no_titles, name2_no_titles):
        return 1.0  # Full match if initials match

    # Check for approximate nickname match
    if approximate_nickname_match(name1_no_titles, name2_no_titles):
        return 1.0  # Full match if approximate nickname match is found

    # Check for missing components
    if has_common_components(name1_no_titles, name2_no_titles):
        return 0.9  # High score if one name contains all components of the other

    # Check for out-of-order components
    if sorted(name1_no_titles.split()) == sorted(name2_no_titles.split()):
        return 0.9  # High score if names have the same components in different order

    # Phonetic similarity
    phonetic_sim = fuzz.ratio(phonetic_encoding(name1_no_titles), phonetic_encoding(name2_no_titles)) / 100.0
    
    # Edit distance similarity (Levenshtein)
    levenshtein_sim = fuzz.ratio(name1_no_titles, name2_no_titles) / 100.0
    
    # Jaro-Winkler similarity
    jaro_winkler_sim = fuzz.token_sort_ratio(name1_no_titles, name2_no_titles) / 100.0

    # Cosine similarity on character n-grams
    vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 3))
    ngram_matrix = vectorizer.fit_transform([name1_no_titles, name2_no_titles])
    cosine_sim = cosine_similarity(ngram_matrix[0:1], ngram_matrix[1:2])[0][0]
    
    # Weighted similarity score
    weighted_score = 0.3 * phonetic_sim + 0.3 * levenshtein_sim + 0.39 * jaro_winkler_sim + 0.4 * cosine_sim
    return weighted_score

# Matching function
def match_names_from_excel(data, threshold=0.6):
    results = []
    for _, row in data.iterrows():
        name1, name2 = row['Name1'], row['Name2']
        score = calculate_similarities(name1, name2)
        label = 1 if score >= threshold else 0
        results.append((name1, name2, score, label))
    
    # Convert results to a DataFrame
    results_df = pd.DataFrame(results, columns=["Name1", "Name2", "Similarity Score", "Predicted Label"])
    return results_df

# Run the matching and store the results
matched_df = match_names_from_excel(data, threshold=0.6)

# Add actual labels to the matched results for comparison
matched_df['Actual Label'] = data['Label']

# Calculate accuracy
accuracy = (matched_df['Predicted Label'] == matched_df['Actual Label']).mean() * 100
precision = precision_score(matched_df['Actual Label'], matched_df['Predicted Label'])
recall = recall_score(matched_df['Actual Label'], matched_df['Predicted Label'])

# Identify mismatches
mismatches = matched_df[matched_df['Predicted Label'] != matched_df['Actual Label']]

# Print mismatched rows
if not mismatches.empty:
    print("Mismatches between Actual and Predicted Labels:")
    print(mismatches)
else:
    print("All predictions match the actual labels.")

# Save mismatches to a separate Excel file
mismatches_file_path = 'mismatched_names_output.xlsx'  # Replace with your desired output file path
mismatches.to_excel(mismatches_file_path, index=False)

# Save results to a new Excel file
output_file_path = 'matched_names_output.xlsx'  # Replace with your desired output file path
matched_df.to_excel(output_file_path, index=False)

print(f"Matched names saved to '{output_file_path}'")
print(f"Mismatched names saved to '{mismatches_file_path}'")
print(f"Accuracy : {accuracy:.2f}%")
print(f"Precision : {precision:.2f}%")
print(f"REcall : {recall:.2f}%")
