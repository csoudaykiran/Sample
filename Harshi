====== Recall+Data preprocessing =====
import pandas as pd
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer
from fuzzywuzzy import fuzz
from metaphone import doublemetaphone

# Load data from Excel
file_path = 'Mathwizzathon_Entity_Matching_Dataset.xlsx'  # Replace with your Excel file path
data = pd.read_excel(file_path)

# Data Preprocessing
data = data.dropna(subset=['Name1', 'Name2'])  # Remove rows where 'Name1' or 'Name2' is missing
data['Name1'] = data['Name1'].str.strip().fillna('')  # Remove leading/trailing whitespace and fill NaNs with empty strings
data['Name2'] = data['Name2'].str.strip().fillna('')  # Remove leading/trailing whitespace and fill NaNs with empty strings

# List of common titles and honorifics to ignore
titles = {'dr', 'mr', 'mrs', 'ms', 'ph.d.', 'professor', 'sir', 'lady', 'captain', 'major'}

# Preprocessing functions
def normalize_name(name):
    return str(name).lower().replace(".", "").replace("-", "").replace(",", "").strip()

def remove_titles(name):
    parts = normalize_name(name).split()
    return ' '.join(part for part in parts if part not in titles)

def phonetic_encoding(name):
    encoded = doublemetaphone(name)
    return encoded[0] if encoded[0] else encoded[1]

def extract_initials(name):
    parts = name.split()
    return [part[0] for part in parts]

def initials_match(name1, name2):
    initials1 = set(extract_initials(name1))
    initials2 = set(extract_initials(name2))
    return initials1 == initials2

def approximate_nickname_match(name1, name2):
    if len(name1) < len(name2):
        shorter, longer = name1, name2
    else:
        shorter, longer = name2, name1

    # Reduced threshold to increase recall
    start_match = fuzz.partial_ratio(shorter, longer) >= 70
    phonetic_match = phonetic_encoding(shorter) == phonetic_encoding(longer[:len(shorter)])
    return start_match or phonetic_match

def has_common_components(name1, name2):
    components1 = set(name1.split())
    components2 = set(name2.split())
    return components1.issubset(components2) or components2.issubset(components1)

# Similarity functions
def calculate_similarities(name1, name2):
    name1, name2 = normalize_name(name1), normalize_name(name2)
    name1_no_titles = remove_titles(name1)
    name2_no_titles = remove_titles(name2)

    # Check for initials and nickname matches
    if initials_match(name1_no_titles, name2_no_titles) or approximate_nickname_match(name1_no_titles, name2_no_titles):
        return 1.0

    if has_common_components(name1_no_titles, name2_no_titles):
        return 0.9

    # Calculate different similarity scores
    phonetic_sim = fuzz.ratio(phonetic_encoding(name1_no_titles), phonetic_encoding(name2_no_titles)) / 100.0
    levenshtein_sim = fuzz.ratio(name1_no_titles, name2_no_titles) / 100.0
    jaro_winkler_sim = fuzz.token_sort_ratio(name1_no_titles, name2_no_titles) / 100.0

    vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 3))
    ngram_matrix = vectorizer.fit_transform([name1_no_titles, name2_no_titles])
    cosine_sim = cosine_similarity(ngram_matrix[0:1], ngram_matrix[1:2])[0][0]

    jaccard_sim = jaccard_similarity(name1_no_titles, name2_no_titles)

    # Adjusted weights to prioritize recall
    weighted_score = 0.25 * phonetic_sim + 0.25 * levenshtein_sim + 0.4 * jaro_winkler_sim + 0.3 * cosine_sim + 0.2 * jaccard_sim

    return weighted_score

def jaccard_similarity(name1, name2):
    set1, set2 = set(name1.split()), set(name2.split())
    return len(set1.intersection(set2)) / len(set1.union(set2))

# Matching function
def match_names_from_excel(data, threshold=0.6):
    results = []
    for _, row in data.iterrows():
        name1, name2 = row['Name1'], row['Name2']
        score = calculate_similarities(name1, name2)
        label = 1 if score >= threshold else 0
        results.append((name1, name2, score, label))
    
    results_df = pd.DataFrame(results, columns=["Name1", "Name2", "Similarity Score", "Predicted Label"])
    return results_df

# Run the matching and store the results
matched_df = match_names_from_excel(data, threshold=0.6)

# Add actual labels to the matched results for comparison
matched_df['Actual Label'] = data['Label']

# Calculate accuracy, precision, and recall
accuracy = (matched_df['Predicted Label'] == matched_df['Actual Label']).mean() * 100
precision = precision_score(matched_df['Actual Label'], matched_df['Predicted Label'])
recall = recall_score(matched_df['Actual Label'], matched_df['Predicted Label'])

# Identify mismatches
mismatches = matched_df[matched_df['Predicted Label'] != matched_df['Actual Label']]

# Print mismatched rows
if not mismatches.empty:
    print("Mismatches between Actual and Predicted Labels:")
    print(mismatches)
else:
    print("All predictions match the actual labels.")

# Save mismatches to a separate Excel file
mismatches_file_path = 'mismatched_names_output.xlsx'
mismatches.to_excel(mismatches_file_path, index=False)

# Save results to a new Excel file
output_file_path = 'matched_names_output.xlsx'
matched_df.to_excel(output_file_path, index=False)

print(f"Matched names saved to '{output_file_path}'")
print(f"Mismatched names saved to '{mismatches_file_path}'")
print(f"Accuracy: {accuracy:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")

==============
# Similarity functions with expanded functionalities
def calculate_similarities(name1, name2):
    name1, name2 = normalize_name(name1), normalize_name(name2)
    name1_no_titles = remove_titles(name1)
    name2_no_titles = remove_titles(name2)

    # Exact match checks with more tolerance
    if initials_match(name1_no_titles, name2_no_titles):
        return 1.0
    if approximate_nickname_match(name1_no_titles, name2_no_titles):
        return 1.0
    if has_common_components(name1_no_titles, name2_no_titles):
        return 0.9
    if sorted(name1_no_titles.split()) == sorted(name2_no_titles.split()):
        return 0.9

    # Additional substring matching
    if len(name1_no_titles) > 3 and len(name2_no_titles) > 3:
        overlap_ratio = fuzz.partial_ratio(name1_no_titles, name2_no_titles) / 100.0
        if overlap_ratio >= 0.75:
            return 0.85  # Score for significant substring overlap

    # Calculate individual similarity scores with adjusted weights
    phonetic_sim = fuzz.ratio(phonetic_encoding(name1_no_titles), phonetic_encoding(name2_no_titles)) / 100.0
    levenshtein_sim = fuzz.ratio(name1_no_titles, name2_no_titles) / 100.0
    jaro_winkler_sim = fuzz.token_sort_ratio(name1_no_titles, name2_no_titles) / 100.0

    # Cosine similarity on character n-grams
    vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 3))
    ngram_matrix = vectorizer.fit_transform([name1_no_titles, name2_no_titles])
    cosine_sim = cosine_similarity(ngram_matrix[0:1], ngram_matrix[1:2])[0][0]

    # Jaccard similarity with enhanced token matching
    jaccard_sim = jaccard_similarity(name1_no_titles, name2_no_titles)

    # Adjusted weighted score to improve recall
    weighted_score = (
        0.25 * phonetic_sim +         # Phonetic similarity
        0.25 * levenshtein_sim +      # Levenshtein similarity
        0.4 * jaro_winkler_sim +      # Jaro-Winkler similarity
        0.35 * cosine_sim +           # Increased cosine similarity
        0.25 * jaccard_sim            # Increased Jaccard similarity
    )

    return weighted_score

========== phoenotic changes=============
def phonetic_matching(name1, name2):
    primary = phonetic_encoding(name1)
    secondary = phonetic_encoding(name2)
    
    # Check with Soundex or other phonetic algorithms
    soundex1 = fuzz.soundex(name1)
    soundex2 = fuzz.soundex(name2)
    
    return (primary == secondary) or (soundex1 == soundex2)

=============feauture extraction and classification using ml=========


import pandas as pd
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from fuzzywuzzy import fuzz
from metaphone import doublemetaphone

# Load data from Excel
file_path = 'Mathwizzathon_Entity_Matching_Dataset.xlsx'  # Replace with your Excel file path
data = pd.read_excel(file_path)

# Data Preprocessing
data = data.dropna(subset=['Name1', 'Name2'])  # Remove rows where 'Name1' or 'Name2' is missing
data['Name1'] = data['Name1'].str.strip().fillna('')  # Remove leading/trailing whitespace and fill NaNs with empty strings
data['Name2'] = data['Name2'].str.strip().fillna('')  # Remove leading/trailing whitespace and fill NaNs with empty strings

# List of common titles and honorifics to ignore
titles = {'dr', 'mr', 'mrs', 'ms', 'ph.d.', 'professor', 'sir', 'lady', 'captain', 'major'}

# Preprocessing functions
def normalize_name(name):
    return str(name).lower().replace(".", "").replace("-", "").replace(",", "").strip()

def remove_titles(name):
    parts = normalize_name(name).split()
    return ' '.join(part for part in parts if part not in titles)

def phonetic_encoding(name):
    encoded = doublemetaphone(name)
    return encoded[0] if encoded[0] else encoded[1]

def extract_features(name1, name2):
    features = {}
    name1_no_titles = remove_titles(name1)
    name2_no_titles = remove_titles(name2)

    features['length_diff'] = abs(len(name1_no_titles) - len(name2_no_titles))
    features['common_components'] = len(set(name1_no_titles.split()).intersection(set(name2_no_titles.split())))
    features['initials_match'] = initials_match(name1_no_titles, name2_no_titles)
    features['phonetic_similarity'] = fuzz.ratio(phonetic_encoding(name1_no_titles), phonetic_encoding(name2_no_titles)) / 100.0
    features['levenshtein_similarity'] = fuzz.ratio(name1_no_titles, name2_no_titles) / 100.0
    features['jaro_winkler_similarity'] = fuzz.token_sort_ratio(name1_no_titles, name2_no_titles) / 100.0

    return features

# Create feature matrix
def create_feature_matrix(data):
    features = []
    for _, row in data.iterrows():
        name1, name2 = row['Name1'], row['Name2']
        feature_dict = extract_features(name1, name2)
        feature_dict['label'] = row['Label']  # Assuming there's a 'Label' column for actual matching labels
        features.append(feature_dict)
    
    return pd.DataFrame(features)

# Create features from the dataset
features_df = create_feature_matrix(data)

# Prepare features and labels
X = features_df.drop('label', axis=1)
y = features_df['label']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Train a logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Calculate precision, recall, and accuracy
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
accuracy = (y_pred == y_test).mean() * 100

# Print the results
print(f"Accuracy: {accuracy:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")

# Save matched results to a new Excel file
output_file_path = 'matched_names_output.xlsx'
features_df.to_excel(output_file_path, index=False)

print(f"Feature data saved to '{output_file_path}'")
====================using random forest=================
import pandas as pd
from sklearn.metrics import precision_score, recall_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity
from fuzzywuzzy import fuzz
from metaphone import doublemetaphone

# Load data from Excel
file_path = 'Mathwizzathon_Entity_Matching_Dataset.xlsx'  # Replace with your Excel file path
data = pd.read_excel(file_path)

# Step 1: Data Preprocessing
data = data.dropna(subset=['Name1', 'Name2'])  # Remove rows where 'Name1' or 'Name2' is missing
data['Name1'] = data['Name1'].str.strip().fillna('')  # Remove leading/trailing whitespace
data['Name2'] = data['Name2'].str.strip().fillna('')  # Remove leading/trailing whitespace

# List of common titles and honorifics to ignore
titles = {'dr', 'mr', 'mrs', 'ms', 'ph.d.', 'professor', 'sir', 'lady', 'captain', 'major'}

# Preprocessing functions
def normalize_name(name):
    return str(name).lower().replace(".", "").replace("-", "").replace(",", "").strip()

def remove_titles(name):
    parts = normalize_name(name).split()
    return ' '.join(part for part in parts if part not in titles)

# Step 2: Feature Engineering
def extract_features(name1, name2):
    features = {}
    name1_no_titles = remove_titles(name1)
    name2_no_titles = remove_titles(name2)

    # Feature 1: Length difference
    features['length_diff'] = abs(len(name1_no_titles) - len(name2_no_titles))

    # Feature 2: Common components count
    features['common_components'] = len(set(name1_no_titles.split()).intersection(set(name2_no_titles.split())))

    # Feature 3: Initials match
    features['initials_match'] = int(initials_match(name1_no_titles, name2_no_titles))

    # Feature 4: Phonetic similarity
    features['phonetic_similarity'] = fuzz.ratio(phonetic_encoding(name1_no_titles), phonetic_encoding(name2_no_titles)) / 100.0

    # Feature 5: Levenshtein similarity
    features['levenshtein_similarity'] = fuzz.ratio(name1_no_titles, name2_no_titles) / 100.0

    # Feature 6: Jaro-Winkler similarity
    features['jaro_winkler_similarity'] = fuzz.token_sort_ratio(name1_no_titles, name2_no_titles) / 100.0

    return features

# Create feature matrix
def create_feature_matrix(data):
    features = []
    for _, row in data.iterrows():
        name1, name2 = row['Name1'], row['Name2']
        feature_dict = extract_features(name1, name2)
        feature_dict['label'] = row['Label']  # Assuming there's a 'Label' column for actual matching labels
        features.append(feature_dict)
    
    return pd.DataFrame(features)

# Create features from the dataset
features_df = create_feature_matrix(data)

# Prepare features and labels
X = features_df.drop('label', axis=1)
y = features_df['label']

# Step 3: Data Splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Model Selection and Training
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Step 5: Prediction
y_pred = model.predict(X_test)

# Step 6: Model Evaluation
accuracy = accuracy_score(y_test, y_pred) * 100
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

# Print the results
print(f"Accuracy: {accuracy:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")

# Save results to a new Excel file if necessary
output_file_path = 'matched_names_output.xlsx'
features_df.to_excel(output_file_path, index=False)

print(f"Feature data saved to '{output_file_path}'")


==========modified=======
import pandas as pd
from sklearn.metrics import precision_score, recall_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import make_scorer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from fuzzywuzzy import fuzz
from metaphone import doublemetaphone

# Load data from Excel
file_path = 'Mathwizzathon_Entity_Matching_Dataset.xlsx'  # Replace with your Excel file path
data = pd.read_excel(file_path)

# Data Preprocessing
data = data.dropna(subset=['Name1', 'Name2'])  # Remove rows where 'Name1' or 'Name2' is missing
data['Name1'] = data['Name1'].str.strip().fillna('')  # Remove leading/trailing whitespace
data['Name2'] = data['Name2'].str.strip().fillna('')  # Remove leading/trailing whitespace

# List of common titles and honorifics to ignore
titles = {'dr', 'mr', 'mrs', 'ms', 'ph.d.', 'professor', 'sir', 'lady', 'captain', 'major'}

# Preprocessing functions
def normalize_name(name):
    return str(name).lower().replace(".", "").replace("-", "").replace(",", "").strip()

def remove_titles(name):
    parts = normalize_name(name).split()
    return ' '.join(part for part in parts if part not in titles)

# Feature Engineering
def extract_features(name1, name2):
    features = {}
    name1_no_titles = remove_titles(name1)
    name2_no_titles = remove_titles(name2)

    # Feature 1: Length difference
    features['length_diff'] = abs(len(name1_no_titles) - len(name2_no_titles))

    # Feature 2: Common components count
    features['common_components'] = len(set(name1_no_titles.split()).intersection(set(name2_no_titles.split())))

    # Feature 3: Initials match
    features['initials_match'] = int(initials_match(name1_no_titles, name2_no_titles))

    # Feature 4: Phonetic similarity
    features['phonetic_similarity'] = fuzz.ratio(phonetic_encoding(name1_no_titles), phonetic_encoding(name2_no_titles)) / 100.0

    # Feature 5: Levenshtein similarity
    features['levenshtein_similarity'] = fuzz.ratio(name1_no_titles, name2_no_titles) / 100.0

    # Feature 6: Jaro-Winkler similarity
    features['jaro_winkler_similarity'] = fuzz.token_sort_ratio(name1_no_titles, name2_no_titles) / 100.0

    return features

# Create feature matrix
def create_feature_matrix(data):
    features = []
    for _, row in data.iterrows():
        name1, name2 = row['Name1'], row['Name2']
        feature_dict = extract_features(name1, name2)
        feature_dict['label'] = row['Label']  # Assuming there's a 'Label' column for actual matching labels
        features.append(feature_dict)
    
    return pd.DataFrame(features)

# Create features from the dataset
features_df = create_feature_matrix(data)

# Prepare features and labels
X = features_df.drop('label', axis=1)
y = features_df['label']

# Data Splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Step 4: Model Selection and Training with class weight adjustment
model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
model.fit(X_train, y_train)

# Step 5: Prediction
y_pred = model.predict(X_test)

# Step 6: Model Evaluation
accuracy = accuracy_score(y_test, y_pred) * 100
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

# Adjusting threshold based on predicted probabilities
y_pred_proba = model.predict_proba(X_test)[:, 1]  # Get the predicted probabilities for the positive class
threshold = 0.6  # Setting threshold to 0.6
y_pred_adjusted = (y_pred_proba >= threshold).astype(int)

# Re-evaluate metrics with adjusted predictions
adjusted_accuracy = accuracy_score(y_test, y_pred_adjusted) * 100
adjusted_precision = precision_score(y_test, y_pred_adjusted)
adjusted_recall = recall_score(y_test, y_pred_adjusted)

# Print the results
print(f"Original Accuracy: {accuracy:.2f}%")
print(f"Original Precision: {precision:.2f}")
print(f"Original Recall: {recall:.2f}")

print(f"Adjusted Accuracy: {adjusted_accuracy:.2f}%")
print(f"Adjusted Precision: {adjusted_precision:.2f}")
print(f"Adjusted Recall: {adjusted_recall:.2f}")

# Save results to a new Excel file if necessary
output_file_path = 'matched_names_output.xlsx'
features_df.to_excel(output_file_path, index=False)

print(f"Feature data saved to '{output_file_path}'")

======================== recall with no ml =================
import pandas as pd
from fuzzywuzzy import fuzz
from metaphone import doublemetaphone
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import accuracy_score, recall_score

# Load data from Excel
file_path = 'Mathwizzathon_Entity_Matching_Dataset.xlsx'  # Replace with your Excel file path
data = pd.read_excel(file_path)

# List of common titles and honorifics to ignore
titles = {'dr', 'mr', 'mrs', 'ms', 'ph.d.', 'professor', 'sir', 'lady', 'captain', 'major'}

# Preprocessing functions
def normalize_name(name):
    name = str(name).lower().replace(".", "").replace("-", "").replace(",", "").strip()
    return ' '.join(name.split())  # Remove extra spaces

def remove_titles(name):
    parts = normalize_name(name).split()
    return ' '.join(part for part in parts if part not in titles)

def phonetic_encoding(name):
    encoded = doublemetaphone(name)
    return (encoded[0], encoded[1])  # Primary and secondary encodings

def soundex_encoding(name):
    name = normalize_name(name)
    soundex_code = name[0].upper()  # Start with the first letter
    counts = {'b': '1', 'f': '1', 'p': '1', 'v': '1',
              'c': '2', 'g': '2', 'j': '2', 'k': '2', 'q': '2', 's': '2', 'x': '2', 'z': '2',
              'd': '3', 't': '3',
              'l': '4',
              'm': '5', 'n': '5',
              'r': '6'}
    
    for char in name[1:]:
        if char in counts:
            code = counts[char]
            if code != soundex_code[-1]:  # Only add if different
                soundex_code += code
    
    soundex_code = soundex_code.ljust(4, '0')  # Pad with zeros
    return soundex_code

def jaccard_similarity(name1, name2):
    set1, set2 = set(name1.split()), set(name2.split())
    return len(set1.intersection(set2)) / len(set1.union(set2))

def ngram_similarity(name1, name2):
    ngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))
    ngram_matrix = ngram_vectorizer.fit_transform([name1, name2])
    similarity = cosine_similarity(ngram_matrix[0:1], ngram_matrix[1:2])
    return similarity[0][0]

# Similarity functions
def calculate_similarities(name1, name2):
    name1_norm, name2_norm = remove_titles(normalize_name(name1)), remove_titles(normalize_name(name2))
    
    # Calculate phonetic similarities
    phonetic1 = phonetic_encoding(name1_norm)
    phonetic2 = phonetic_encoding(name2_norm)
    
    phonetic_match = int(phonetic1[0] == phonetic2[0] or phonetic1[1] == phonetic2[1])
    soundex_match = int(soundex_encoding(name1_norm) == soundex_encoding(name2_norm))
    
    # Calculate fuzzy similarities
    levenshtein_sim = fuzz.ratio(name1_norm, name2_norm) / 100.0
    jaro_winkler_sim = fuzz.token_sort_ratio(name1_norm, name2_norm) / 100.0
    jaccard_sim = jaccard_similarity(name1_norm, name2_norm)
    ngram_sim = ngram_similarity(name1_norm, name2_norm)
    
    # Use a threshold to determine if the names are similar
    thresholds = [0.80, 0.85, 0.75, 0.70, 0.70]  # Adjusted thresholds for each measure

    # Match if any similarity score exceeds the threshold
    is_match = (phonetic_match or soundex_match or
                levenshtein_sim >= thresholds[0] or
                jaro_winkler_sim >= thresholds[1] or
                jaccard_sim >= thresholds[2] or
                ngram_sim >= thresholds[3])

    return int(is_match)

# Prepare results
results = []
for _, row in data.iterrows():
    name1, name2, label = row['Name1'], row['Name2'], row['Label']
    match = calculate_similarities(name1, name2)
    results.append({'Name1': name1, 'Name2': name2, 'Match': match, 'Label': label})

# Create a DataFrame for results
results_df = pd.DataFrame(results)

# Calculate performance metrics
y_true = results_df['Label']  # Actual labels
y_pred = results_df['Match']   # Predicted matches

# Calculate accuracy and recall
accuracy = accuracy_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)

# Print performance metrics
print(f"Accuracy: {accuracy:.2f}")
print(f"Recall: {recall:.2f}")

# Save the results to Excel
output_file_path = 'ml_matched_names_output.xlsx'
results_df.to_excel(output_file_path, index=False)

print(f"Results saved to '{output_file_path}'")



